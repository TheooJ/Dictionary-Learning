{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-34a34f37-059e-45f5-bed0-58452b664fab",
    "deepnote_cell_type": "markdown",
    "id": "pe3N_NYnX3oW"
   },
   "source": [
    "# **Compressed Sensing 2021 Project : Dictionary Learning**\n",
    "Hugo Chardon & ThÃ©o Jolivet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-aba25720-4682-42c1-b38c-0672b8ecfee1",
    "deepnote_cell_type": "markdown",
    "id": "7GTEXhceacQ-"
   },
   "source": [
    "## 1 General framework of Dictionary Learning\n",
    "\n",
    "### 1.1 One dictionary to learn them all\n",
    "\n",
    "In the classical setting of dictionary learning, we consider a signal $\\mathbf{x}\\in \\mathbb{R}^N$ and we want to represent it in a basis in which it has a sparse representation. The key idea is to write $\\mathbf{x}$ as a linear combination of $k$ other vectors lying in $\\mathbb{R}^N$, called _atoms_ of a predefined _dictionary_. Such a dictionary will be denoted by a matrix $\\mathbf{D}=[\\mathbf{d}_1, \\dots, \\mathbf{d}_k ] \\in \\mathbb{R}^{N\\times k}$, and we want to find the coefficients $\\alpha_1, \\dots, \\alpha_k$ such that $\\sum_{i=1}^k \\alpha_i \\mathbf{d}_i$ is relatively close to $\\mathbf{x}$ in $\\ell_2$-norm. More precisely, we aim at minimizing : \n",
    "\n",
    "$$\n",
    "\\underset{\\boldsymbol{\\alpha} \\in \\mathbb{R}^k}{\\min} \\ \\dfrac{1}{2} \\lVert \\mathbf{x} - \\mathbf{D} \\boldsymbol{\\alpha} \\rVert_2^2\n",
    "$$\n",
    "\n",
    "In order to enforce a sparse representation of $\\mathbf{x}$ in such a dictionary, we penalize the $\\ell_1$-norm of the $\\boldsymbol{\\alpha}$'s as in the LASSO estimator. We denote $l(\\mathbf{x},\\mathbf{D})$ the optimal value of the $\\ell_1$-_sparse coding_ problem :\n",
    "\n",
    "$$\n",
    "l(\\mathbf{x},\\mathbf{D}) = \\underset{\\boldsymbol{\\alpha} \\in \\mathbb{R}^k}{\\min} \\ \\dfrac{1}{2} \\lVert \\mathbf{x}-\\mathbf{D}\\boldsymbol{\\alpha}\\lVert_2^2 \\ + \\ \\lambda\\lVert\\boldsymbol{\\alpha}\\lVert_1\n",
    "$$\n",
    "\n",
    "To compute the dictionary, we usually have access to $n$ observations following a distribution $p(\\mathbf{x})$, stored in $\\mathbf{X} = [\\mathbf{x}_1, \\dots ,\\mathbf{x}_n] \\in \\mathbb{R}^{N \\times n} $, and we then minimize the _empirical risk_ $f_n(\\mathbf{D}) = \\dfrac{1}{n} \\sum_{i=1}^n l(\\mathbf{x}_i,\\mathbf{D})$. Recall that we aim at minimizing the _expected  risk_ $f(\\mathbf{D}) = \\mathbb{E}_{\\mathbf{x}}[l(\\mathbf{x},\\mathbf{D})]$ which is linked to $f_n$ by $f(\\mathbf{D}) = \\underset{n \\to + \\infty}{\\lim} f_n(\\mathbf{D})$.\n",
    "\n",
    "Famous examples of such dictionaries that are widely used in signal processing are the wavelets or Fourier basis, in which we can find a sparse representation of $\\mathbf{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-8c06a22e-cfb1-41c8-9400-e97e6aae5019",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 1.2 Adaptive Dictionary\n",
    "\n",
    "Notable applications in signal processing (denoising, inpainting, etc...) and unsupervised data modeling (recommender systems, document clustering, etc...) called for more scalable and efficient matrix-factorization methods. Adaptive dictionaries were thus introduced in recent years to learn the dictionary instead of using a predefined one, and the optimization now seeks to find the coefficients $\\boldsymbol{\\alpha}$ as well as the dictionary $\\mathbf{D}$.\n",
    "\n",
    "To avoid having arbitrarily large values for the columns of the dictionary (and thus an arbitrarily small $\\boldsymbol{\\alpha}$), we only consider dictionaries which columns have a $\\ell_2$-norm of one at most, that is, dictionaries in the set $\\mathcal{C} = \\{ \\mathbf{D} \\in \\mathbb{R}^{N \\times k} \\ \\text{s.t.} \\ \\forall j = 1,...,k; \\ \\mathbf{d}_j^T \\mathbf{d}_j \\leq 1 \\}$. We can then rewrite the problem as a joint optimization problem : \n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{D} \\in \\mathcal{C}, \\ \\alpha \\in \\mathbb{R}^{k \\times n}}{\\min} \\ \\dfrac{1}{n} \\sum_{i=1}^n (\\dfrac{1}{2} \\lVert\\mathbf{x}_i - \\mathbf{D} \\boldsymbol{\\alpha}_i \\lVert_2^2 \\ + \\ \\lambda \\lVert\\boldsymbol{\\alpha}_i\\lVert_1)\n",
    "$$\n",
    "\n",
    "which is convex with respect to both variables when the other is fixed. We optimize the objective function with respect to both the dictionary $\\mathbf{D} \\in \\mathbb{R}^{N \\times k}$ and the coefficients $\\boldsymbol{\\alpha} = [\\boldsymbol{\\alpha}_1,...,\\boldsymbol{\\alpha}_n] \\in \\mathbb{R}^{k \\times n}$. Usually, $k$ is a multiple of 2, such as 256, 512 or 1024. Dictionaries with $k>N$ are called _overcomplete_ dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00023-f616508e-5706-48e3-82ae-e982526a5958",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## 2 Simultaneous dictionary learning and classification (based on _Supervised Dictionary Learning_,  NeurIPS '08)\n",
    "\n",
    "We introduce here the core idea presented in the article. Rather than first creating a sparse representation of the data through dictionary representation and then performing a classical task such as classification on the projected data, the authors propose to perform both tasks simultaneously. Without loss of generality, we will focus on binary classification (generalization is not difficult but the core ideas would remain the same with added technicalities).\n",
    "\n",
    "### 2.1 Standard classification task\n",
    "\n",
    "We have $n$ data points $\\mathbf{x}_1, ..., \\mathbf{x}_n$ endowed with labels $y_i = \\pm 1$. We want to learn a function $f(\\mathbf{x}, \\boldsymbol{\\alpha}, \\boldsymbol{\\theta})$ (namely, the parameter $\\boldsymbol{\\theta}$) generally referred to as a soft classifier, such that $\\forall i$, $\\mathrm{sgn}(f(\\mathbf{x}_i)) = y_i$. The parameter $\\boldsymbol{\\theta}$ can be learned as the result of the following optimization procedure :\n",
    "\n",
    "$$\n",
    "\\underset{\\boldsymbol{\\theta}}{\\min} \\ \\sum_{i=1}^n c(y_i f(\\mathbf{x_i}, \\boldsymbol{\\alpha_i}, \\boldsymbol{\\theta})) + \\lambda_2 \\lVert \\boldsymbol{\\theta} \\rVert_2^2\n",
    "$$\n",
    "\n",
    "with $c(u)= \\log(1+e^{-u})$ the logistic loss (convex surrogate of the 0-1 loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-6124c109-13f3-4dce-a42d-bd276850d8e7",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 2.2 Simultaneous learning\n",
    "\n",
    "We create a mixed objective function that has both a classification loss component and a dictionary learning one.\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{D}, \\boldsymbol{\\theta}, \\boldsymbol{\\alpha}}{\\min} \\ \\sum_{i=1}^n \\left(c(y_i f(\\mathbf{x_i}, \\boldsymbol{\\alpha_i}, \\boldsymbol{\\theta})) + \\lambda_0 \\lVert \\mathbf{x}_i - \\mathbf{D} \\boldsymbol{\\alpha}_i \\rVert_2^2 + \\lambda_1 \\lVert \\boldsymbol{\\alpha}_i \\rVert_1\\right) + \\lambda_2 \\lVert \\boldsymbol{\\theta} \\rVert_2^2 \\qquad (*)\n",
    "$$\n",
    "\n",
    "which can be rewritten as \n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{D}, \\boldsymbol{\\theta}}{\\min} \\ \\sum_{i=1}^n \\mathcal{S}^*(\\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, y_i)\n",
    "$$\n",
    "\n",
    "with \n",
    "\n",
    "$$\n",
    "\\mathcal{S}^*(\\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, y_i) = \\underset{\\boldsymbol{\\alpha}}{\\min} \\ \\mathcal{S}(\\boldsymbol{\\alpha}_i, \\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, y_i)\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\mathcal{S}(\\boldsymbol{\\alpha}_i, \\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, y_i) = c(y_i f(\\mathbf{x_i}, \\boldsymbol{\\alpha_i}, \\boldsymbol{\\theta})) + \\lambda_0 \\lVert \\mathbf{x}_i - \\mathbf{D} \\boldsymbol{\\alpha}_i \\rVert_2^2 + \\lambda_1 \\lVert \\boldsymbol{\\alpha}_i \\rVert_1\n",
    "$$\n",
    "\n",
    "Note that $\\mathcal{S}^*(\\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, y_i)$ is the loss for a single pair of observation $(\\mathbf{x}_i, y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-9dd0005c-2795-4397-a05c-a98f106055c6",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### A more discriminative approach\n",
    "\n",
    "\n",
    "In order to make the overall model more disriminative between the two classes we want to make our dictionary such that the loss for a wrong prediction $\\mathcal{S}^*(\\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, -y_i)$ is bigger than the loss for a correct prediction $\\mathcal{S}^*(\\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, y_i)$. The key point here being that the loss itself depends on the dictionary $\\mathbf{D}$ and the codes $\\boldsymbol{\\alpha}_i$'s. We formulate this new idea in the following optimization problem : \n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{D}, \\boldsymbol{\\theta}}{\\min} \\ \\sum_{i=1}^n c\\left(\\mathcal{S}^*(\\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, -y_i) - \\mathcal{S}^*(\\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, y_i)\\right) + \\lambda_2 \\lVert \\boldsymbol{\\theta} \\rVert^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-2724b4fb-a338-4cfe-8194-0fdd1ac99e2e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### The trade-off\n",
    "\n",
    "This last problem is harder to solve than $(*)$ so we compromise between the two and introduce a trade-off parameter $\\mu$ and solve a composite optimization problem :\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{D}, \\boldsymbol{\\theta}}{\\min} \\ \\sum_{i=1}^n \\left[ \\mu \\, c\\left(\\mathcal{S}^*(\\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, -y_i) - \\mathcal{S}^*(\\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, y_i)\\right) + (1-\\mu) \\, \\mathcal{S}^*(\\mathbf{x}_i, \\mathbf{D}, \\boldsymbol{\\theta}, y_i)\\right] + \\lambda_2 \\lVert \\boldsymbol{\\theta} \\rVert^2 \\qquad (**)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-c0272b82-4d70-4492-a29f-50bf20cd354f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### From learnt dictionary to classification\n",
    "\n",
    "Once we have learnt both the dictionary and the classifier's parameters, we use them to predict the labels of new (or test) examples. given a new example $\\mathbf{x}$, its label is predicted by comparing $\\mathcal{S}^*(\\mathbf{x}, \\mathbf{D}, \\boldsymbol{\\theta}, -1)$ and $\\mathcal{S}^*(\\mathbf{x}, \\mathbf{D}, \\boldsymbol{\\theta}, -1)$. the attributed label is the one for which $\\mathcal{S}^*$ is the lowest. Therefore, we note that a new prediction requires to compute the dictionary representation of the new example : what we refer to as _supervised sparse coding_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-722eb9d0-7cd7-4428-a5aa-fffe1f22eebc",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 2.3 Implementation\n",
    "\n",
    "Solving the optimization problem $(**)$ is carried out by alternate block coordinate descent. We alternate between :\n",
    "\n",
    "(i) _Supervised sparse coding_ : this stept aims at computing the representations $\\boldsymbol{\\alpha}_i$'s of the datapoints $\\mathbf{x}_i$ in the dictionary $\\mathbf{D}$ and the classifier's parameter $\\boldsymbol{\\theta}$ fixed. This step amounts to computing the above-defined $\\mathcal{S^*}$ and the $\\boldsymbol{\\alpha}$ that minimizes it, for both $y_i$ and $-y_i$.\n",
    "\n",
    "(ii) _Supervised dictionary update_ : this time, the $\\boldsymbol{\\alpha}_i$'s are fixed and we want to update the dictionary $\\mathbf{D}$ and the classifier's parameter $\\boldsymbol{\\theta}$.\n",
    "\n",
    "\n",
    "Regarding the step $(i)$ in the algorithm described above, the authors suggest using a method known as _fixed-point continuation method_ which is a very general framework for this class of optimization problems. We propose here to compare two approaches : one using the python library cvxpy (in which the optimization method used is not specified) and a _manual_ method (that is, an optimization algorithm written from scratch and inspired by the LASSO implementation of coordinate subgradient descent).\n",
    "\n",
    "Hereafter we describe and implement the algorithm _Supervised Dictionary Learning_ proposed by the authors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00003-d8a03d15-4fa0-45e6-b988-1c0471d0f7ec",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 90932,
    "execution_start": 1616624910269,
    "is_code_hidden": false,
    "output_cleared": false,
    "source_hash": "46ea2166",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: cvxpy==1.1.11 in /opt/anaconda3/lib/python3.7/site-packages (1.1.11)\r\n",
      "Requirement already satisfied: ecos>=2 in /opt/anaconda3/lib/python3.7/site-packages (from cvxpy==1.1.11) (2.0.7.post1)\r\n",
      "Requirement already satisfied: scs>=1.1.6 in /opt/anaconda3/lib/python3.7/site-packages (from cvxpy==1.1.11) (2.1.2)\r\n",
      "Requirement already satisfied: osqp>=0.4.1 in /opt/anaconda3/lib/python3.7/site-packages (from cvxpy==1.1.11) (0.6.2.post0)\r\n",
      "Requirement already satisfied: numpy>=1.15 in /opt/anaconda3/lib/python3.7/site-packages (from cvxpy==1.1.11) (1.19.5)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/anaconda3/lib/python3.7/site-packages (from cvxpy==1.1.11) (1.3.1)\r\n",
      "Requirement already satisfied: qdldl in /opt/anaconda3/lib/python3.7/site-packages (from osqp>=0.4.1->cvxpy==1.1.11) (0.1.5.post0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install cvxpy==1.1.11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00009-22a7b49c-a5c1-431a-bb89-a81b01d2758f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 8,
    "execution_start": 1616625129708,
    "source_hash": "18ac30d2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxpy as cp\n",
    "from numpy import random as rd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, validation_curve\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-ef91d8cc-0a7a-4f29-87ae-00800929229b",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In the cell below we define some elementary functions and an optimization procedure used to compute the step $(i)$ (Supervised sparse coding) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00005-39aac472-a829-42ec-ac83-527e95fdbee8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17,
    "execution_start": 1616632616040,
    "source_hash": "ca17dff8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supervised sparse coding\n",
    "\n",
    "def loss(x, D, w, b, alpha, y, lambda0=1e-3):\n",
    "    # w and b are the linear classifier parameters. they are fixed for this step \n",
    "    return cp.logistic(y * (cp.sum(cp.multiply(w, alpha)) + b)) + lambda0 * cp.norm2(x - D @ alpha) **2\n",
    "\n",
    "\n",
    "def regularizer(alpha):\n",
    "    return cp.norm1(alpha)\n",
    "\n",
    "\n",
    "def objective(x, D, w, b, alpha, y, lambda0=1e-3, lambda1=1e-3):\n",
    "    return loss(x, D, w, b, alpha, y, lambda0) + lambda1*regularizer(alpha)\n",
    "\n",
    "\n",
    "# Computing the vector alpha\n",
    "# This function will be used in the main algorithm for the alphas update step \n",
    "def supervised_sparse_coding(x, D, w, b, y, reconstruction_param, regularization_param, require_opt_value=False):\n",
    "    \"\"\" Supervised sparse coding.\n",
    "\n",
    "        :param x: the observation of which we want the dictionary representation\n",
    "        :param D: dictionary\n",
    "        :param w: weights of the function f\n",
    "        :param b: bias of the function f\n",
    "        :param y: labels of the observations\n",
    "        :param reconstruction_param: reconstruction error parameter\n",
    "        :param regularization_param: l1 regularization parameter\n",
    "        :return: coefficients alpha (if require_opt_value is False) or minimal value of the objective (if require_opt_value is True)\n",
    "    \"\"\"\n",
    "    k = D.shape[1] # Number of atoms in the dictionary = number of columns in D\n",
    "    alpha = cp.Variable(k)\n",
    "    lambda0 = cp.Parameter(nonneg=True)\n",
    "    lambda1 = cp.Parameter(nonneg=True)\n",
    "\n",
    "    problem = cp.Problem(cp.Minimize(objective(x, D, w, b, alpha, y, lambda0, lambda1)))\n",
    "\n",
    "    lambda0.value = reconstruction_param\n",
    "    lambda1.value = regularization_param\n",
    "\n",
    "    problem.solve(solver=cp.SCS, max_iters=5000)\n",
    "    #problem.solve(solver='CVXOPT')\n",
    "\n",
    "    # In some cases we need the optimal value S^*, in some other cases we need the optimal alpha (i.e. min or argmin)\n",
    "\n",
    "    if require_opt_value:\n",
    "        return problem.value\n",
    "    else:\n",
    "        return alpha.value \n",
    "\n",
    "\n",
    "def make_prediction(x, D, w, b, reconstruction_param, regularization_param):\n",
    "    pos_loss = supervised_sparse_coding(x, D, w, b, 1, reconstruction_param, regularization_param, require_opt_value=True)\n",
    "    neg_loss = supervised_sparse_coding(x, D, w, b, -1, reconstruction_param, regularization_param, require_opt_value=True)\n",
    "\n",
    "    predicted_label = 1 if pos_loss < neg_loss else -1 # Pseudo-classifier as a plug-in\n",
    "    \n",
    "    return predicted_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00011-c5605bc7-3dab-40fc-a237-a6abaf90e6f6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1199,
    "execution_start": 1616632616891,
    "source_hash": "49a10f43",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03299411 -0.00970407 -0.01344096 -0.07050355  0.01311588 -0.05308313\n",
      "  0.03594842  0.02468213 -0.01767859  0.02404816 -0.04614584 -0.00510889\n",
      " -0.02180906 -0.05125683 -0.00949303  0.01191196  0.02976438 -0.03591195\n",
      " -0.01294414  0.0112366   0.03805752  0.01270062  0.05218876 -0.01849839\n",
      "  0.01541124 -0.07021649 -0.05144567  0.07620739  0.01796868 -0.03553514\n",
      "  0.05977095 -0.05373793 -0.02098304 -0.0025736   0.02041398 -0.00148395\n",
      "  0.01754517  0.01622728  0.01823912 -0.03495948 -0.04683056 -0.0049973\n",
      " -0.05583253  0.03053311 -0.05227728  0.00077734  0.03696736  0.05694654\n",
      " -0.02542474 -0.00054826 -0.0215384   0.02579079  0.01548698  0.03537026\n",
      " -0.07837095 -0.04317113  0.01345617 -0.0512667   0.00740554  0.03798713\n",
      " -0.00214323 -0.07288477  0.0322389   0.01010449 -0.01621971  0.01710337\n",
      " -0.03543298  0.00806604 -0.03065715 -0.08467754  0.0095439  -0.02352549\n",
      "  0.02525961 -0.0061754  -0.01982632 -0.00033903  0.00411864  0.07358591\n",
      " -0.01087762 -0.00865604  0.01018361 -0.03364956 -0.00917868 -0.04992165\n",
      "  0.00160134 -0.00543111  0.01474408 -0.08401224  0.02689976 -0.05298507\n",
      "  0.05261958  0.11252719  0.0273528   0.00760781 -0.02736294 -0.0094214\n",
      "  0.00341045  0.02250638  0.00409811  0.03752632]\n",
      "quelconque :  102403.82768138958\n",
      "optimal :  857.8587198701309\n"
     ]
    }
   ],
   "source": [
    "# Test of alphas update\n",
    "\n",
    "N = 1000\n",
    "k = 100\n",
    "\n",
    "test_vector = rd.randn(N)\n",
    "dictionary = rd.randn(N,k)\n",
    "\n",
    "w = rd.randn(k)\n",
    "b=rd.randn()\n",
    "\n",
    "lambda0_test = 0.5\n",
    "lambda1_test = 0.15 * lambda0_test\n",
    "\n",
    "alpha_opt = supervised_sparse_coding(x=test_vector, D=dictionary, w=w, b=b, y=1, reconstruction_param=lambda0_test, regularization_param=lambda1_test)\n",
    "print(alpha_opt)\n",
    "\n",
    "alpha_quelconque = rd.randn(k)\n",
    "\n",
    "print('quelconque : ', np.sum((test_vector - dictionary @ alpha_quelconque)**2))\n",
    "print('optimal : ', np.sum((test_vector - dictionary @ alpha_opt)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-4ec2a82a-26ce-4003-906d-d92322e3ec29",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "This is a toy example for the computation of $\\boldsymbol{\\alpha}$'s. We want to know if we learn the right coefficients for a random dictionary, and we compare them with random coefficients. The reconstruction error is lower with the optimized coefficients, showing that the learnt coefficients (alpha_opt) perform better at the reconstruction task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-db0bd068-d982-426b-9fce-c9c6163c303c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Then we define the functions needed to compute step $(ii)$ (Supervised dictionary update). We will perform the update of $\\mathbf{D}$ and $\\boldsymbol{\\theta}$ by solving the above stated minimization problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00009-b8c330ba-de97-413e-8a46-dda78f6ebc58",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1616632624781,
    "source_hash": "9bede379",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Supervised dictionary update\n",
    "\n",
    "def logistic(u):\n",
    "    return np.log(1 + np.exp(-u))\n",
    "\n",
    "\n",
    "def deriv_logistic(u):\n",
    "    return - 1 / (1 + np.exp(u)) \n",
    "\n",
    "\n",
    "def cal_S(alpha, x, D, w, b, y, lambda0, lambda1):\n",
    "    return logistic(y * (np.dot(w, alpha) + b)) + lambda0 * np.linalg.norm(x - D @ alpha) ** 2 + lambda1 * np.linalg.norm(alpha, ord=1)\n",
    "\n",
    "\n",
    "def compute_omega(z, mu, alpha_star_neg, alpha_star_pos, x, D, w, b, y):\n",
    "    S_neg = cal_S(alpha_star_neg, x, D, w, b, -y, lambda0, lambda1)\n",
    "    S_pos = cal_S(alpha_star_pos, x, D, w, b, y, lambda0, lambda1)\n",
    "    omega = - mu * z * deriv_logistic(S_neg - S_pos)\n",
    "    if z==y:\n",
    "        omega += 1-mu\n",
    "    return omega\n",
    "\n",
    "\n",
    "def compute_gradient(X, D, w, b, y, alpha_star_neg, alpha_star_pos, mu, lambda0, lambda1):\n",
    "    '''\n",
    "    :param X: the whole dataset given as a matrix. here we assume that each data point is a column of X so we have to be careful\n",
    "            regarding the potential need to transpose the input matrix\n",
    "    :param D: dictionary\n",
    "    :param w: weights of the classifier\n",
    "    :param b: bias of the classifier\n",
    "    :param y: labels associated with each observation, {-1,1} convention\n",
    "    :param alpha_star_neg: best alpha if the label was negative\n",
    "    :param alpha_star_neg: best alpha if the label was positive\n",
    "    :param mu: classification/generation tradeoff\n",
    "    :param lambda0: reconstruction regularization parameter\n",
    "    :param lambda1: l1 regularization parameter\n",
    "    :return: partial gradients with respect to dictionary, weights, bias\n",
    "    '''\n",
    "\n",
    "    partial_grad_dict = np.zeros_like(D)\n",
    "    partial_grad_weights = np.zeros_like(w)\n",
    "    partial_grad_bias = np.zeros_like(b)\n",
    "    n = X.shape[0]  \n",
    "\n",
    "    for i in range(n):\n",
    "        omega_i_neg = compute_omega(-1, mu, alpha_star_neg, alpha_star_pos, X[i], D, w, b, y[i])\n",
    "        omega_i_pos = compute_omega(1, mu, alpha_star_neg, alpha_star_pos, X[i], D, w, b, y[i])\n",
    "\n",
    "        partial_grad_dict += omega_i_pos * np.outer(X[i] - D @ alpha_star_pos, alpha_star_pos) + omega_i_neg * np.outer(X[i] - D @ alpha_star_neg, alpha_star_neg)\n",
    "        partial_grad_weights += omega_i_pos * deriv_logistic(np.dot(w, alpha_star_pos) + b) * alpha_star_pos - omega_i_neg * deriv_logistic(np.dot(w, alpha_star_neg) + b) * alpha_star_neg\n",
    "        partial_grad_bias += omega_i_pos * deriv_logistic(np.dot(w, alpha_star_pos) + b) - omega_i_neg * deriv_logistic(np.dot(w, alpha_star_neg) + b)\n",
    "\n",
    "    partial_grad_dict *= -2 * lambda0\n",
    "    \n",
    "    return partial_grad_dict, partial_grad_weights, partial_grad_bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-0073239d-b2cf-4a30-815c-72763a5c9d94",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "To perform part $(ii)$ described above, we also use a projected gradient descent. The projection is only done for the dictionaries, by normalizing its columns if necessary (that is replacing the $j$-th column $\\mathbf{d}_j$ by $\\frac{\\mathbf{d}_j}{\\lVert \\mathbf{d}_j \\rVert_2}$ if $\\lVert \\mathbf{d}_j \\rVert_2 > 1$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00014-4a8f8550-12a7-4c5f-8a03-cf42fc52fee9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1616632626437,
    "source_hash": "d999b20a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# (projected) gradient descent on D, w and b (theta is just a packing of (w,b))\n",
    "\n",
    "def update_parameters(X, D, w, b, y, alpha_star_neg, alpha_star_pos, mu, lambda0, lambda1, maxiter=20, lr=0.5):\n",
    "    for t in trange(maxiter):\n",
    "        partial_grad_dict, partial_grad_weights, partial_grad_bias = compute_gradient(X, D, w, b, y, alpha_star_neg, alpha_star_pos, mu, lambda0, lambda1)\n",
    "        b -= lr * partial_grad_bias\n",
    "        w -= lr * partial_grad_weights\n",
    "        D -= lr * partial_grad_dict\n",
    "        \n",
    "        k = D.shape[1] # Number of atoms in the dictionary\n",
    "        for j in range(k):\n",
    "            norm_j = np.linalg.norm(D[:,j])\n",
    "            if norm_j > 1:\n",
    "                D[:, j] /= norm_j\n",
    "        \n",
    "        return D, w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-56ab8079-567c-42ab-8001-fce467b52526",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Now we can implement the main algorithm, which basically amounts to looping over different values of the trade-off parameter $\\mu$ and then iterating until convergence, alternating between $(i)$ and $(ii)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00014-ff509c51-4ae3-4d82-b3e7-b4f89e45d49f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2,
    "execution_start": 1616632628006,
    "source_hash": "6794242",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global function combining steps (i) and (ii)\n",
    "\n",
    "def supervised_dictionnary_learning(mus, data, labels, nb_atoms, lambda0, lambda1, lambda2, global_maxiter):\n",
    "    ''' Supervised_dictionnary_learning.\n",
    "        Learns a dictionary, the coefficients, and a classifier.\n",
    "\n",
    "    :param mus: array of parameters for classification/generation tradeoff\n",
    "    :param data: the training set. We assume that each datapoint is a column of X so X has a shape size of observations x number of observations\n",
    "    :param labels: labels assoociated with each observation, {-1,1} convention\n",
    "    :param nb_atoms: columns of the dictionary\n",
    "    :param lambda0: reconstruction regularization parameter\n",
    "    :param lambda1: l1 regularization parameter\n",
    "    :param lambda2: l2 regularization parameter\n",
    "    :param global_maxiter: max number of iterations\n",
    "    :return: best dictionary, best parameter and best precision\n",
    "    '''\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data, labels) # Split the training set\n",
    "    N = X_train.shape[1]\n",
    "\n",
    "    precisions_record = []\n",
    "    thetas = []\n",
    "    dictionaries = []\n",
    "\n",
    "    w = np.zeros(nb_atoms)\n",
    "    b = 0.\n",
    "    D = rd.randn(N,nb_atoms)\n",
    "\n",
    "    # Normalize the columns \n",
    "    for j in range(nb_atoms):\n",
    "        D[:,j] /= np.linalg.norm(D[:,j])\n",
    "\n",
    "    for mu in mus:\n",
    "        for t in range(global_maxiter):\n",
    "            for i,x in enumerate(X_train):\n",
    "                alpha_star_neg = supervised_sparse_coding(x, D, w, b, -1, reconstruction_param=lambda0, regularization_param=lambda1)\n",
    "                alpha_star_pos = supervised_sparse_coding(x, D, w, b, 1, reconstruction_param=lambda0, regularization_param=lambda1)\n",
    "            \n",
    "            D, w, b = update_parameters(X_train, D, w, b, y_train, alpha_star_neg, alpha_star_pos, mu, lambda0, lambda1)\n",
    "\n",
    "        y_test_predict = np.zeros_like(y_test)\n",
    "\n",
    "        for i,x in enumerate(X_test):\n",
    "            y_test_predict[i] = make_prediction(x)\n",
    "        \n",
    "        confusion_matrix_test = confusion_matrix(y_test,y_test_predict)\n",
    "        tp = confusion_matrix_test[0,0]\n",
    "        fp = confusion_matrix_test[1,0]\n",
    "        precision = tp/(tp+fp)\n",
    "\n",
    "        precisions_record.append(precision)\n",
    "        thetas.append((w,b))\n",
    "        dictionaries.append(D)\n",
    "\n",
    "    best_precision = max(precisions_record)\n",
    "    best_index = precisions.index(best_precision)\n",
    "    best_parameter = thetas[best_index]\n",
    "    best_dictionary = dictionaries[best_index]\n",
    "\n",
    "    return best_dictionary, best_parameter, best_precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00021-5364de75-e5bd-4f22-ba54-e0bb10f9ea67",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 162505,
    "execution_start": 1616632633898,
    "output_cleared": false,
    "source_hash": "1412953e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/cvxpy/problems/problem.py:1246: UserWarning: Solution may be inaccurate. Try another solver, adjusting the solver settings, or solve with verbose=True for more information.\n",
      "  \"Solution may be inaccurate. Try another solver, \"\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  \n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: RuntimeWarning: overflow encountered in exp\n",
      "  after removing the cwd from sys.path.\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/20 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-25c4cf02d36d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mglobal_maxiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mbest_dictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_parameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassif_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupervised_dictionnary_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_atoms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_atoms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_maxiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobal_maxiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-567e93c3bc9d>\u001b[0m in \u001b[0;36msupervised_dictionnary_learning\u001b[0;34m(mus, data, labels, nb_atoms, lambda0, lambda1, lambda2, global_maxiter)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0malpha_star_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupervised_sparse_coding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruction_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0malpha_star_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msupervised_sparse_coding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreconstruction_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlambda1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_star_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha_star_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-34f060126f23>\u001b[0m in \u001b[0;36msupervised_sparse_coding\u001b[0;34m(x, D, w, b, y, reconstruction_param, regularization_param, require_opt_value)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mlambda1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularization_param\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m#problem.solve(solver='CVXOPT')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0msolve_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msolve_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, solver, warm_start, verbose, gp, qcp, requires_grad, enforce_dpp, **kwargs)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         data, solving_chain, inverse_data = self.get_problem_data(\n\u001b[0;32m--> 919\u001b[0;31m             solver, gp, enforce_dpp, verbose)\n\u001b[0m\u001b[1;32m    920\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36mget_problem_data\u001b[0;34m(self, solver, gp, enforce_dpp, verbose)\u001b[0m\n\u001b[1;32m    589\u001b[0m                     \u001b[0;34mf'{reduction_chain_str}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                 )\n\u001b[0;32m--> 591\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolving_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    592\u001b[0m             safe_to_cache = (\n\u001b[1;32m    593\u001b[0m                 \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/reductions/chain.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, problem, verbose)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Applying reduction {type(r).__name__}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0minverse_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/reductions/dcp2cone/cone_matrix_stuffing.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, problem)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoeffExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m         params_to_objective, flattened_variable = self.stuffed_objective(\n\u001b[0;32m--> 312\u001b[0;31m             problem, extractor)\n\u001b[0m\u001b[1;32m    313\u001b[0m         \u001b[0;31m# Lower equality and inequality to Zero and NonNeg.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mcons\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/reductions/dcp2cone/cone_matrix_stuffing.py\u001b[0m in \u001b[0;36mstuffed_objective\u001b[0;34m(self, problem, extractor)\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstuffed_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Extract to c.T * x + r; c is represented by a ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mboolean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_mip_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/utilities/coeff_extractor.py\u001b[0m in \u001b[0;36maffine\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mexpr_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpr_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mnum_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpr_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mop_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonical_form\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpr_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/utilities/coeff_extractor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mexpr_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dpp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpr_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0mnum_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpr_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mop_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonical_form\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpr_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/atoms/atom.py\u001b[0m in \u001b[0;36mis_dpp\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \"\"\"\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dcp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dcp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dgp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dgp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/utilities/performance_utils.py\u001b[0m in \u001b[0;36m_compute_once\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/expressions/expression.py\u001b[0m in \u001b[0;36mis_dcp\u001b[0;34m(self, dpp)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdpp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mscopes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdpp_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_convex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_concave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_convex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_concave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/utilities/performance_utils.py\u001b[0m in \u001b[0;36m_compute_once\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/atoms/atom.py\u001b[0m in \u001b[0;36mis_convex\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_atom_convex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 if not (arg.is_affine() or\n\u001b[0m\u001b[1;32m    174\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_convex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_incr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                         (arg.is_concave() and self.is_decr(idx))):\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/utilities/performance_utils.py\u001b[0m in \u001b[0;36m_compute_once\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/expressions/expression.py\u001b[0m in \u001b[0;36mis_affine\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"Is the expression affine?\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_constant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_convex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_concave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/utilities/performance_utils.py\u001b[0m in \u001b[0;36m_compute_once\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/atoms/atom.py\u001b[0m in \u001b[0;36mis_concave\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_atom_concave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m                 if not (arg.is_affine() or\n\u001b[0m\u001b[1;32m    191\u001b[0m                         \u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_concave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_incr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                         (arg.is_convex() and self.is_decr(idx))):\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/cvxpy/utilities/performance_utils.py\u001b[0m in \u001b[0;36m_compute_once\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m      \"\"\"\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mcache_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'__cache__'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch the global algorithm\n",
    "\n",
    "mus = np.linspace(0,1,10)\n",
    "data = load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "y = 2 * y - 1\n",
    "nb_atoms = 24\n",
    "lambda0 = 0.5\n",
    "lambda1 = 0.15 * lambda0\n",
    "lambda2 = 1.2e-3\n",
    "global_maxiter = 10\n",
    "\n",
    "best_dictionary, best_parameter, classif_precision = supervised_dictionnary_learning(mus, data=X, labels=y, nb_atoms=nb_atoms, lambda0=lambda0, lambda1=lambda1, lambda2=lambda2, global_maxiter = global_maxiter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-586214ee-61d8-47f7-92c9-a3e350b1c2a4",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Note that we cannot directly compare dictionaries because there are an infinity of linear combinations such that $\\mathbf{x} \\approx \\sum_{i=1}^k \\alpha_i \\mathbf{d}_i$ . We can however plot the columns of the dictionaries, for instance in the case of character recognition, a \"7\" is an horizontal bar, an angle and a slantwise bar. We can generalize this idea for more complex images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-5a7fd57b-8cfb-4ef4-acde-694a15c42715",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 2.4 Running the algorithm on real data\n",
    "\n",
    "We are going to run the SDL algorithm on a medical dataset. The breast cancer diagnostic data set has 30 prediction variables and a binary output : the indicator that the individual was diagnosed with breast cancer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cell_id": "00018-32fb9bfd-2c30-4edd-9b98-ef181dd1e9ed",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 90,
    "execution_start": 1616623324455,
    "source_hash": "219823c5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n",
      " 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n",
      " 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n",
      " 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n",
      " 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n",
      " 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Presentation of the dataset\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "print(y)\n",
    "N = X.shape[1]\n",
    "N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-6054c68a-bc57-4d8b-8e11-5670913be7f8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "We now split data between train and test sets. not only is it necessary to assess the quality of the leart model but we also need to evaluate each model obtained for each value of $\\mu$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "00021-0910a496-a61b-4a35-86e0-2a3ecd52744e",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1616634741376,
    "source_hash": "fa7d0593",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n"
     ]
    }
   ],
   "source": [
    "# Splitting of the data\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-ec945a23-6154-427b-8b97-9a74aad70d4a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Classification benchmark : SVM\n",
    "\n",
    "We first perform the classification task using a classical Support Vector Machine (SVM). For this, we will use the SVM class already available in sklearn as a benchmark.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "00025-91d5c809-4b97-49b8-b7c7-8e36512c91b8",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 32769,
    "execution_start": 1616634760492,
    "source_hash": "138f5b56",
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "validation_curve() takes 3 positional arguments but 5 positional arguments (and 1 keyword-only argument) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7219ca9fc89d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mC_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLinearSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"C\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_scores_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mvalid_scores_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: validation_curve() takes 3 positional arguments but 5 positional arguments (and 1 keyword-only argument) were given"
     ]
    }
   ],
   "source": [
    "# Plot learning and test scores\n",
    "\n",
    "C_range = np.logspace(-6,-2,50)\n",
    "train_scores, valid_scores = validation_curve(LinearSVC(), X_train, y_train, \"C\", C_range, cv=10)\n",
    "train_scores_mean = np.mean(train_scores,axis=1)\n",
    "valid_scores_mean = np.mean(valid_scores,axis=1)\n",
    "plt.plot(np.log10(C_range),train_scores_mean,label=\"learning scores\")\n",
    "plt.plot(np.log10(C_range),valid_scores_mean,label=\"testing scores\")\n",
    "plt.legend()\n",
    "plt.xlabel('log(C)')\n",
    "plt.ylabel('score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "00024-91fc686f-148a-4ca1-820e-ee8bbe0a7e46",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 100,
    "execution_start": 1616634744115,
    "source_hash": "ae1dbf5c",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'svm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-00eb11c4e267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtuned_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'C'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msvm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuned_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'svm' is not defined"
     ]
    }
   ],
   "source": [
    "# Cross-validation to find the best regularization parameter\n",
    "\n",
    "tuned_parameters = [{'C': [1, 5, 10, 50, 100, 500, 1000]}]\n",
    "\n",
    "clf = GridSearchCV(svm, tuned_parameters, scoring='precision')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00026-bb407a2e-475a-49ac-98ff-97ebb4b85cee",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 122,
    "execution_start": 1616634796958,
    "source_hash": "492cb17",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9230769230769231\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/svm/_base.py:986: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Perform SVM with best value\n",
    "C_best = C_range[np.argmax(valid_scores_mean)]\n",
    "svm = LinearSVC(C=C_best).fit(X_train,y_train)\n",
    "print('Score:', svm.score(X_test,y_test))\n",
    "y_test_predict = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00027-8287b95e-5a11-4035-a292-752f2d0cf102",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 12,
    "execution_start": 1616634800128,
    "source_hash": "e406aabf",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[39 10]\n",
      " [ 1 93]]\n",
      "Recall : 0.7959183673469388\n",
      "Precision : 0.975\n"
     ]
    }
   ],
   "source": [
    "# Print relevant metrics\n",
    "\n",
    "confusion_matrix_test = confusion_matrix(y_test,y_test_predict)\n",
    "print(confusion_matrix_test)\n",
    "\n",
    "tp = confusion_matrix_test[0,0]\n",
    "fp = confusion_matrix_test[1,0]\n",
    "fn = confusion_matrix_test[0,1]\n",
    "recall = tp/(tp+fn)\n",
    "precision = tp/(tp+fp)\n",
    "print('Recall :', recall)\n",
    "print('Precision :', precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-e3edb2f9-b042-425c-88bf-4d32dc4dcd73",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Benchmarking results\n",
    "This will then be our benchmark : we fine-tuned a SVM by cross-validating the regularization parameter C and reached a precision of 97,5% on the breast cancer dataset. Let us find out how the SDL compares with this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-000f57e1-935c-4567-89c8-a7ab674e2d84",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## 3 Online dictionary learning (based on ICML '09)\n",
    "\n",
    "In this section, we do not consider the classification problem like in Section 2 and we are instead interested in purely _reconstructive_ dictionaries. We consider the online setting, where we dynamically learn the dictionary, setting which is interesting when we consider video streams for instance, where frames come one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-1a05b446-6049-481c-88cc-a5bef859dfc9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 3.1 Online dictionary learning\n",
    "\n",
    "In a similar way as in Section 1, we consider the problem : \n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{D} \\in \\mathcal{C}, \\ \\alpha \\in \\mathbb{R}^{k \\times n}}{\\min} \\ \\dfrac{1}{n} \\sum_{i=1}^n (\\dfrac{1}{2} \\lVert\\mathbf{x}_i - \\mathbf{D} \\boldsymbol{\\alpha}_i \\lVert_2^2 \\ + \\ \\lambda \\lVert\\boldsymbol{\\alpha}_i\\lVert_1) \\qquad (*)\n",
    "$$ \n",
    "\n",
    "The difference here is that at each time step $t=1,...,T$, we sample a signal $\\mathbf{x}_t$ and adapt the dictionary based on this \"new\" observation. This can be useful for instance if the dataset size is too large, or if information is available in a dynamic way.\n",
    "\n",
    "In our online learning setting, we denote :\n",
    "\n",
    "$\\mathbf{A} = [\\mathbf{a}_1, ..., \\mathbf{a}_k] \\in \\mathbb{R}^{k \\times k} = \\sum_{i=1}^t \\boldsymbol{\\alpha}_i \\boldsymbol{\\alpha}_i^\\top$ \n",
    "\n",
    "$\\mathbf{B} = [\\mathbf{b}_1, ..., \\mathbf{b}_k] \\in \\mathbb{R}^{k \\times k} = \\sum_{i=1}^t \\mathbf{x}_i \\boldsymbol{\\alpha}_i^\\top$\n",
    "\n",
    "These matrices will be helpful when we want to update the dictionary by leveraging the nature of the problem.\n",
    "\n",
    "\n",
    "The online dictionary learning algorithm solves (*) by drawing a sample $\\boldsymbol{x}_t$ and iterating between 2 steps :\n",
    "\n",
    "(i) _Sparse coding_ : $\\mathbf{D}_{t-1}$ is fixed, and we compute $\\boldsymbol{\\alpha}_t$ with LARS. We update \n",
    "\n",
    "$\\mathbf{A}_t \\leftarrow \\mathbf{A}_{t-1} + \\boldsymbol{\\alpha}_t \\boldsymbol{\\alpha}_t^\\top $\n",
    "\n",
    "$\\mathbf{B}_t \\leftarrow \\mathbf{B}_{t-1} + \\mathbf{x}_t \\boldsymbol{\\alpha}_t^\\top $\n",
    "\n",
    "(ii) _Dictionary update_ : $\\boldsymbol{\\alpha}_t$ is fixed, we solve \n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\mathbf{D}_t & = & \\underset{\\mathbf{D} \\in \\mathcal{C}}{\\min} \\ \\dfrac{1}{t} \\sum_{i=1}^t (\\dfrac{1}{2} \\lVert\\mathbf{x}_i - \\mathbf{D} \\boldsymbol{\\alpha}_i \\lVert_2^2 \\ + \\ \\lambda \\lVert\\boldsymbol{\\alpha}_i\\lVert_1) \\\\ \\\\\n",
    " & = & \\dfrac{1}{t}(\\dfrac{1}{2} Tr(\\mathbf{D}^\\top \\mathbf{D} \\mathbf{A}_t) - Tr(\\mathbf{D}^\\top \\mathbf{B}_t) )\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is to say for $j=1,...,k$ do :\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\mathbf{u}_j & \\leftarrow & \\dfrac{1}{\\mathbf{A}_{jj}}(\\mathbf{b}_j - \\mathbf{D} \\mathbf{a}_j) + \\mathbf{d}_j\\\\ \n",
    "\\mathbf{d}_j & \\leftarrow & \\dfrac{1}{\\text{max}(\\lVert \\mathbf{u}_j \\lVert_2, 1)} \\mathbf{u}_j\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "As opposed to batch methods, this method takes advantage of the structure of the data to significantly reduce computation time. The dictionary is updated using $\\mathbf{D}_{t-1}$ as a warm restart because the dictionaries don't change a lot between updates. Note that we could also consider a _mini-batch setting_, where we sample multiple $\\mathbf{x}_t$ each step. In the paper, the authors found that it considerably reduced computation time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-20893b59-5986-419b-a749-83aeda5da606",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### 3.2  Gradient descent to compute the dictionary\n",
    "\n",
    "Usually, the dictionary is updated using classical _stochastic gradient descent_, which for the update of $\\mathbf{D}$ reads : \n",
    "\n",
    "$\\mathbf{D}_{t} = \\Pi_\\mathcal{C}[\\mathbf{D}_{t-1} \\ - \\ \\dfrac{\\rho}{t} \\nabla_\\mathbf{D} l(\\mathbf{x},\\mathbf{D}_{t-1})]$\n",
    "\n",
    "$$ 2 l (\\mathbf{x}, \\mathbf{D+H}, \\boldsymbol{\\alpha}) = \\lVert \\mathbf{x} - (\\mathbf{D}+\\mathbf{H})\\boldsymbol{\\alpha} \\rVert^2 = \\lVert \\mathbf{x} - \\mathbf{D} \\boldsymbol{\\alpha} \\rVert^2 - 2 \\langle \\mathbf{x} - \\mathbf{D}\\boldsymbol{\\alpha}, \\mathbf{H}\\boldsymbol{\\alpha} \\rangle + \\lVert \\mathbf{H}\\boldsymbol{\\alpha} \\rVert^2 $$\n",
    "\n",
    "and noting that \n",
    "\n",
    "$$ \\langle \\mathbf{x} - \\mathbf{D}\\boldsymbol{\\alpha}, \\mathbf{H}\\boldsymbol{\\alpha} \\rangle = \\langle (\\mathbf{x} - \\mathbf{D}\\boldsymbol{\\alpha})\\boldsymbol{\\alpha}^\\top , \\mathbf{H} \\rangle_F $$\n",
    "\n",
    "we get that \n",
    "\n",
    "$$ \\nabla_\\mathbf{D} l (\\mathbf{x}, \\mathbf{D}, \\boldsymbol{\\alpha}) = (\\mathbf{D}\\boldsymbol{\\alpha} - \\mathbf{x})\\boldsymbol{\\alpha}^\\top $$.\n",
    "\n",
    "For this last equality we used a simple \"trick\" : if $\\mathbf{u} \\in \\mathbb{R}^p$, $\\mathbf{v} \\in \\mathbb{R}^q$, and $\\mathbf{M}$ is a $p\\times q$ matrix, then we can rewrite the scalar product \n",
    "$$\n",
    "\\langle \\mathbf{u}, \\mathbf{M}\\mathbf{v} \\rangle = \\sum_{i=1}^p \\sum_{j=1}^q u_i M_{i,j} v_j\n",
    "$$\n",
    "\n",
    "as\n",
    "\n",
    "$$\n",
    "\\sum_{i,j} u_i v_j M_{i,j}\n",
    "$$\n",
    "\n",
    "which is the standard Frobenius scalar product $\\langle \\mathbf{u} \\mathbf{v}^\\top, \\mathbf{M} \\rangle_F$ between the matrices $\\mathbf{u} \\mathbf{v}^\\top$ and $\\mathbf{M}$.\n",
    "\n",
    "We also implement and test this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00010-23933e16-3f9f-43f6-b33b-add6eab37706",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1616625772993,
    "source_hash": "42a52e4e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00009-14375aac-f03f-45be-9522-1532a3164e0c",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1616635395209,
    "source_hash": "61bcc95f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Online Dictionary Learning for Sparse Coding (ICML 2009)\n",
    "\n",
    "def dictionary_update(D, A, B):\n",
    "    \"\"\" Return the updated dictionary.\n",
    "\n",
    "        :param D: dictionary\n",
    "        :param A: matrix of alpha*alpha^T\n",
    "        :param B: matrix of x*alpha^T\n",
    "        :return: updated dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    D_new = np.zeros_like(D)\n",
    "    for i in range(D.shape[1]):\n",
    "        D_new[:,i] = 1 / A[i,i] * (B[:,i] - D @ A[:,i]) + D[:,i]\n",
    "        D_new[:,i] = 1 / max(np.linalg.norm(D_new[:,i]),1) * D_new[:,i] # Normalize the columns of the dictionary\n",
    "\n",
    "    return D_new\n",
    "\n",
    "\n",
    "def online_dictionary_learning(X, k, T=200):\n",
    "    \"\"\" Online dictionary learning.\n",
    "\n",
    "        :param x: samples of signal to reconstruct\n",
    "        :param k: number of atoms in the dictionary\n",
    "        :param T: number of iterations\n",
    "        :return: learned dictionary\n",
    "    \"\"\"\n",
    "    D = rd.randn(X.shape[1],k)  \n",
    "    A = np.zeros((k,k))\n",
    "    B = np.zeros((X.shape[1],k))\n",
    "\n",
    "    iis = np.random.randint(0, X.shape[0], T)\n",
    "\n",
    "    lambd = 1.2/np.sqrt(X.shape[1])\n",
    "    \n",
    "    reg = linear_model.Lasso(lambd, max_iter=5000)\n",
    "\n",
    "    for i in range(T): \n",
    "        X_sample = X[iis[i]] # Loop over T random samples\n",
    "        \n",
    "        reg.fit(D, X_sample) # Update alpha\n",
    "        alpha = reg.coef_\n",
    "\n",
    "        A += np.outer(alpha, alpha)\n",
    "        B += np.outer(X_sample, alpha)\n",
    "\n",
    "        D = dictionary_update(D, A, B) # Update D\n",
    "    \n",
    "    return D, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00037-da701d9c-495c-45a6-9e76-aa52b3cdd996",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1004,
    "execution_start": 1616635396327,
    "source_hash": "f70d0fa5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Validation with breast cancer data\n",
    "\n",
    "k = 6\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, _, _ = train_test_split(X, y) # Split the training set\n",
    "\n",
    "Dictionary, coefficients = online_dictionary_learning(X_train, k, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00040-3c3da400-72fc-4c80-9532-53084b0db84b",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17,
    "execution_start": 1616635400222,
    "source_hash": "aca50c73",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392.5427595203566\n",
      "1022.729247118921\n"
     ]
    }
   ],
   "source": [
    "# Comparison of results with a random representation of the signal\n",
    "\n",
    "random_coeffs = rd.randn(k)\n",
    "print(np.linalg.norm(X_test[10] - Dictionary @ coefficients))\n",
    "print(np.linalg.norm(X_test[10] - Dictionary @ random_coeffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-9bfa24ff-a8e9-4526-9f71-879d0e01303e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Our coefficients indeed perform better than random coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00040-1a131a70-8f28-4b29-9408-7da1c602464f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1616635487568,
    "source_hash": "2bd9ea06",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Stochastic gradient descent\n",
    "\n",
    "def grad_i(i, X, D, alpha):\n",
    "    return np.outer(D @ alpha - X[i], alpha)\n",
    "\n",
    "\n",
    "def sgd(D_init, alpha, D, grad_i, maxiter=1000, step=1., store_every=10):\n",
    "    \"\"\" Stochastic gradient descent algorithm (with projection)\n",
    "\n",
    "        :param D_init: initial value of D before the descent\n",
    "        :param alpha: vector of coefficients\n",
    "        :param D: dictionary\n",
    "        :param grad_i: gradient on a single observations\n",
    "        :param n_iter: maximum number of iterations\n",
    "        :param step: learning rate\n",
    "        :param store_every: frequency of storing x\n",
    "        :return: last updated D and array of previous D during descent\n",
    "    \"\"\"\n",
    "    D = D_init.copy()\n",
    "    D_list = []\n",
    "\n",
    "    iis = np.random.randint(0, X.shape[0], maxiter) # To sample observations\n",
    "\n",
    "    for i in range(maxiter):\n",
    "        D = D - step / np.sqrt(i + 1) * grad_i(iis[i], X, D, alpha)\n",
    "        \n",
    "        for i in range(D.shape[1]):\n",
    "            D[:,i] = 1 / max(np.linalg.norm(D[:,i]),1) * D[:,i] # Normalize the columns of the dictionary\n",
    "\n",
    "        # Store metrics after store_every iterations.\n",
    "        if i % store_every == 0:\n",
    "            D_list.append(D.copy())\n",
    "    return D, D_list\n",
    "\n",
    "\n",
    "def stochastic_dictionary_learning(X, k, T=200):\n",
    "    \"\"\" Stochastic dictionary learning.\n",
    "\n",
    "        :param X: samples of signal to reconstruct\n",
    "        :param k: number of atoms in the dictionary\n",
    "        :param T: number of iterations\n",
    "        :return: learned dictionary\n",
    "    \"\"\"\n",
    "    D = rd.randn(X.shape[1],k)  \n",
    "    A = np.zeros((k,k))\n",
    "    B = np.zeros((X.shape[1],k))\n",
    "\n",
    "    iis = np.random.randint(0, X.shape[0], T)\n",
    "\n",
    "    lambd = 1.2/np.sqrt(X.shape[1])\n",
    "    \n",
    "    reg = linear_model.Lasso(lambd, max_iter=5000)\n",
    "\n",
    "    for i in range(T): \n",
    "        X_sample = X[iis[i]] # Loop over T random samples\n",
    "        \n",
    "        reg.fit(D, X_sample) # Update alpha\n",
    "        alpha = reg.coef_\n",
    "\n",
    "        D,_ = sgd(D, alpha, D, grad_i, maxiter=100, step=1., store_every=10) # Update D\n",
    "    \n",
    "    return D, alpha\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00041-9a9f6a7d-339a-4874-9727-4dd6711ec0d4",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 15717,
    "execution_start": 1616635550855,
    "source_hash": "56cb46be",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 21379.334078966174, tolerance: 308.3552399966679\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 910.2924841282002, tolerance: 194.7394088631043\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 7120.390280246036, tolerance: 99.1880996592234\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 303.94262603728566, tolerance: 106.19842390867281\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17598.195730912325, tolerance: 495.57448840125846\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 757708.0772815999, tolerance: 1287.6319100396454\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2518.3146677283657, tolerance: 429.2236564287113\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3977.0064070122316, tolerance: 1065.7455477367887\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 45.530326467662235, tolerance: 21.46732875418354\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 329.87005754825805, tolerance: 150.7306561767708\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 48.68280184303876, tolerance: 47.417738790688716\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 8012.6986621588585, tolerance: 423.2949933860533\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 255.6974004922813, tolerance: 27.47531163241179\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 232106.55591167696, tolerance: 386.1632104654074\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1094.62443457685, tolerance: 406.6248044882509\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 87593.55374507688, tolerance: 364.72806320708753\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1418.6569340609713, tolerance: 210.69294763226185\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 16145.342690188205, tolerance: 423.2949933860533\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 645.599003508767, tolerance: 40.657127411455136\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2679.0884603944432, tolerance: 181.05581781606912\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 70441.15402979532, tolerance: 364.34538142184493\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 596.6814754973893, tolerance: 516.8020179417338\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 109809.77541077486, tolerance: 447.0962319125104\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 164224.3886353171, tolerance: 429.2236564287113\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 63.40836675502942, tolerance: 52.76766860573143\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 376.7557367887348, tolerance: 345.0802090147291\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 78.02733780058043, tolerance: 63.18390039190292\n",
      "  positive)\n",
      "/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:532: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 435677.0054192338, tolerance: 662.1288254511596\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "k = 6\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, _, _ = train_test_split(X, y) # Split the training set\n",
    "\n",
    "Dictionary_stoch, coefficients_stoch = stochastic_dictionary_learning(X_train, k, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00042-155dcce9-6acb-4723-b090-a48cae977875",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 105,
    "execution_start": 1616635579304,
    "source_hash": "54cc5ad5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181.9502451841986\n",
      "496.9527580041181\n"
     ]
    }
   ],
   "source": [
    "# Comparison of results with a random representation of the signal\n",
    "\n",
    "random_coeffs = rd.randn(k)\n",
    "print(np.linalg.norm(X_test[10] - Dictionary_stoch @ coefficients_stoch))\n",
    "print(np.linalg.norm(X_test[10] - Dictionary_stoch @ random_coeffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-d56fd179-88d2-44a0-8699-7e41566b20f4",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## 4. Some insights into more recent uses of dictionary learning\n",
    "\n",
    "### From euclidean dictionaries to general dictionaries\n",
    "\n",
    "If we forget for a moment the $\\ell_1$ penalty, the remaining objective function that we want to minimize to compute the representation of a single vector $\\mathbf{x}$ with respect to a dictionary $\\mathbf{D}$ is\n",
    "\n",
    "$$\n",
    "\\underset{\\alpha_1, \\dots, \\alpha_k}{\\min} \\lVert \\mathbf{x} - \\sum_{i=1}^k \\alpha_i \\mathbf{d}_i \\rVert^2\n",
    "$$\n",
    "\n",
    "Which can be interpreted as  \"finding the barycenter of atoms $\\mathbf{d}_1, \\dots, \\mathbf{d}_k$ that is closest to $\\mathbf{x}$ with respect to the euclidean distance. This being said, we can replace the euclidean distance with any other distance that will better take into account the natural geometry of the problem at hand, and that geometry does not have to be the natural euclidean geometry. This is how we can generalize the idea of dictionary learning to any other type of data lying in a metric space $(\\mathcal{E}, d)$. In such a setting the coefficients are computed in two steps : given some coefficients $\\alpha_1, \\dots, \\alpha_k$, the barycenter of the atoms is defined as \n",
    "\n",
    "$$\n",
    "\\mathbf{d}^*(\\alpha_1, \\dots, \\alpha_k) = \\underset{\\mathbf{d} \\in \\mathcal{E}}{\\arg \\min} \\sum_{i=1}^k \\alpha_i \\, d(\\mathbf{d}, \\mathbf{d}_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00041-0d445b67-3b6e-4000-85bf-2e565b11dcd6",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "then the optimal coefficients for a point $\\mathbf{x}$ are defined as \n",
    "\n",
    "$$\n",
    "\\alpha_1^*, \\dots, \\alpha_k^* = \\underset{\\alpha_1, \\dots, \\alpha_k}{\\arg \\min} d(\\mathbf{x}, \\mathbf{d}^*(\\alpha_1, \\dots, \\alpha_k))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-02ac23a0-6270-4397-bc4e-6b59fc7055e5",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Dictionaries in Topological Data Analysis\n",
    "\n",
    "One of the core tools in topological data analysis (TDA) is the persistence diagram. It is an object that encapsulates a simple topological signature of complex objects, typically compact submanifolds of $\\mathbb{R}^d$ Here the sparsity lies in the fact that we can represent some very complicated topological objects by a much simpler _summary_. The point here is not to delve into the details of TDA, but to show a less straightforward use of dictionary learning. One of the key ideas of TDA is to only store compact representations of data instead of the data itself. This idea is viable due to the fact that persistence diagrams enjoy a lot of stability properties that makes them \"good\" representations of the data in some sense. Once the space of persistence diagrams is endowed with a metric, it is then possible to define the notion of dictionaries and mixing coefficients just like in the classical euclidean setting, with the general notion of dictionary and coefficients explained above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-f1fed19c-0098-496c-9eb3-8f6695efa886",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<img src=\"persistence_diagrams.jpg\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-de25d542-725e-43f9-86e7-bedeb47e70a9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00017-998b4146-044b-47aa-b554-8ae05b2050d2",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The space of persistence diagrams can be endowed with a family of Wasserstein-like distance functions. A diagram can be seen as a discrete measure in $\\mathbb{R}^2$ so it can be equiped with the classical $p$-Wasserstein distances $W_p$. \n",
    "\n",
    "<img src=\"distance_between_diagrams.jpg\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-1608334b-7d04-48e3-8cb9-f75a3f3d5b8d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "the two illustrations above were taken from Frederic Chazal's course (see https://geometrica.saclay.inria.fr/team/Fred.Chazal/M2Orsay2021.htm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00047-0e9a89e1-2b97-4bc2-9fa0-092053affc45",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In dictionary learning, the goal is to find a _sparse_ representation of a vector $\\mathbf{x}$ in a dictionary $\\mathbf{D}$. That is to say, $\\mathbf{x} \\approx \\sum_{i=1}^k \\alpha_i \\mathbf{d}_i$, and we aim at finding both the dictionary $\\mathbf{D}$ and the coefficients $\\boldsymbol{\\alpha}$.\n",
    "We presented two algorithms for adaptive dictionary learning : supervised dictionary learning and online dictionary learning. The first one aims at learning both a classifier and the dictionary with the associated coefficients, while the latter propose an alternative to batch methods which could be too computationally intensive in high-dimension. We fully implemented the two algorithms and showed that the second one was able to learn the sparse representation of $\\mathbf{x}$. Our first algorithm works on a toy example but takes too long to converge with a real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=df08becc-1aed-46b8-9e35-e2c2ba6bc296' target=\"_blank\">\n",
    "<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Compressed Sensing - Chardon Jolivet.ipynb",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "e772703b-90a3-433c-afdf-5f98bc800cb6",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
